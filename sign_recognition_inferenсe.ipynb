{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image,  ImageEnhance\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report,  confusion_matrix\n",
    "\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attachment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>length</th>\n",
       "      <th>train</th>\n",
       "      <th>frame_count</th>\n",
       "      <th>height_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5ce6ce8a-d191-47d4-97b4-3fc6f9138a73</td>\n",
       "      <td>я</td>\n",
       "      <td>185bd3a81d9d618518d10abebf0d17a8</td>\n",
       "      <td>1920</td>\n",
       "      <td>1080</td>\n",
       "      <td>91.0</td>\n",
       "      <td>True</td>\n",
       "      <td>51</td>\n",
       "      <td>1920_1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c9e2dbd1-ec2f-4382-a80e-5bca8396a9b1</td>\n",
       "      <td>я</td>\n",
       "      <td>46dd04a1caa75ed3082b573cb5a3ad26</td>\n",
       "      <td>1920</td>\n",
       "      <td>1080</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>58</td>\n",
       "      <td>1920_1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3f72244c-7379-4403-b7ee-e1b1b8d78d46</td>\n",
       "      <td>я</td>\n",
       "      <td>db573f94204e56e0cf3fc2ea000e5bdc</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>49.0</td>\n",
       "      <td>True</td>\n",
       "      <td>49</td>\n",
       "      <td>1280_720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5adf08e2-4ff6-4183-ba22-446c1cd0b0f8</td>\n",
       "      <td>я</td>\n",
       "      <td>0211b488644476dd0fec656ccb9b74fc</td>\n",
       "      <td>1920</td>\n",
       "      <td>1080</td>\n",
       "      <td>50.0</td>\n",
       "      <td>True</td>\n",
       "      <td>50</td>\n",
       "      <td>1920_1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13a148fc-a546-47ab-b5d0-f9d0c3cce1de</td>\n",
       "      <td>я</td>\n",
       "      <td>2d84da20c251acaeb3186642fcb04f2e</td>\n",
       "      <td>1920</td>\n",
       "      <td>1080</td>\n",
       "      <td>27.0</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "      <td>1920_1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>74db1174-1314-43c7-bb8a-cc5cce5e2510</td>\n",
       "      <td>редко</td>\n",
       "      <td>f2dc6eab563f93d86629c1cfe479f09d</td>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "      <td>56.0</td>\n",
       "      <td>False</td>\n",
       "      <td>56</td>\n",
       "      <td>1440_1440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>5d742908-98ce-4172-bb2f-02f3595860a4</td>\n",
       "      <td>редко</td>\n",
       "      <td>798aa826b6129d33f72f62d4ba60b681</td>\n",
       "      <td>720</td>\n",
       "      <td>1280</td>\n",
       "      <td>61.0</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>720_1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>dfe2ff4c-98fa-4ead-b861-f499f570a321</td>\n",
       "      <td>месяц</td>\n",
       "      <td>798aa826b6129d33f72f62d4ba60b681</td>\n",
       "      <td>720</td>\n",
       "      <td>1280</td>\n",
       "      <td>58.0</td>\n",
       "      <td>False</td>\n",
       "      <td>58</td>\n",
       "      <td>720_1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>04856a31-8326-49cb-8e0c-0843c64a1d92</td>\n",
       "      <td>время</td>\n",
       "      <td>798aa826b6129d33f72f62d4ba60b681</td>\n",
       "      <td>720</td>\n",
       "      <td>1280</td>\n",
       "      <td>52.0</td>\n",
       "      <td>False</td>\n",
       "      <td>52</td>\n",
       "      <td>720_1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>f0ac4a5e-c438-4f39-a577-2814f423afa7</td>\n",
       "      <td>вчера</td>\n",
       "      <td>3dd2ce2659aada17b976390004ebe322</td>\n",
       "      <td>1920</td>\n",
       "      <td>886</td>\n",
       "      <td>35.0</td>\n",
       "      <td>False</td>\n",
       "      <td>35</td>\n",
       "      <td>1920_886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             attachment_id   text  \\\n",
       "0     5ce6ce8a-d191-47d4-97b4-3fc6f9138a73      я   \n",
       "1     c9e2dbd1-ec2f-4382-a80e-5bca8396a9b1      я   \n",
       "2     3f72244c-7379-4403-b7ee-e1b1b8d78d46      я   \n",
       "3     5adf08e2-4ff6-4183-ba22-446c1cd0b0f8      я   \n",
       "4     13a148fc-a546-47ab-b5d0-f9d0c3cce1de      я   \n",
       "...                                    ...    ...   \n",
       "1995  74db1174-1314-43c7-bb8a-cc5cce5e2510  редко   \n",
       "1996  5d742908-98ce-4172-bb2f-02f3595860a4  редко   \n",
       "1997  dfe2ff4c-98fa-4ead-b861-f499f570a321  месяц   \n",
       "1998  04856a31-8326-49cb-8e0c-0843c64a1d92  время   \n",
       "1999  f0ac4a5e-c438-4f39-a577-2814f423afa7  вчера   \n",
       "\n",
       "                               user_id  height  width  length  train  \\\n",
       "0     185bd3a81d9d618518d10abebf0d17a8    1920   1080    91.0   True   \n",
       "1     46dd04a1caa75ed3082b573cb5a3ad26    1920   1080    58.0   True   \n",
       "2     db573f94204e56e0cf3fc2ea000e5bdc    1280    720    49.0   True   \n",
       "3     0211b488644476dd0fec656ccb9b74fc    1920   1080    50.0   True   \n",
       "4     2d84da20c251acaeb3186642fcb04f2e    1920   1080    27.0   True   \n",
       "...                                ...     ...    ...     ...    ...   \n",
       "1995  f2dc6eab563f93d86629c1cfe479f09d    1440   1440    56.0  False   \n",
       "1996  798aa826b6129d33f72f62d4ba60b681     720   1280    61.0  False   \n",
       "1997  798aa826b6129d33f72f62d4ba60b681     720   1280    58.0  False   \n",
       "1998  798aa826b6129d33f72f62d4ba60b681     720   1280    52.0  False   \n",
       "1999  3dd2ce2659aada17b976390004ebe322    1920    886    35.0  False   \n",
       "\n",
       "      frame_count height_width  \n",
       "0              51    1920_1080  \n",
       "1              58    1920_1080  \n",
       "2              49     1280_720  \n",
       "3              50    1920_1080  \n",
       "4              27    1920_1080  \n",
       "...           ...          ...  \n",
       "1995           56    1440_1440  \n",
       "1996           61     720_1280  \n",
       "1997           58     720_1280  \n",
       "1998           52     720_1280  \n",
       "1999           35     1920_886  \n",
       "\n",
       "[2000 rows x 9 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = 'slovo'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train_100')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test_100')\n",
    "ann_100 = pd.read_csv(os.path.join(DATA_DIR, 'annotations_100.csv'))\n",
    "ann_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Holistic object to detect pose, face, and hands keypoints\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# Drawing utilities\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image: {image}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "\n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the video file\n",
    "video_path = './благодарю_cut.mp4'\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "output_path = './благодарю_new.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        # Draw landmarks on the frame\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Write the frame with landmarks to the output video file\n",
    "        out.write(image)\n",
    "\n",
    "        # Display the frame (optional)\n",
    "        cv2.imshow('Frame with Landmarks', image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация MediaPipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# Определение вспомогательных функций\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def adjust_landmarks(arr, center):\n",
    "    arr_reshaped = arr.reshape(-1, 3)\n",
    "    center_repeated = np.tile(center, (len(arr_reshaped), 1))\n",
    "    arr_adjusted = arr_reshaped - center_repeated\n",
    "    arr_adjusted = arr_adjusted.reshape(-1)\n",
    "    return arr_adjusted\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    nose = pose[:3]\n",
    "    lh_wrist = lh[:3]\n",
    "    rh_wrist = rh[:3]\n",
    "    pose_adjusted = adjust_landmarks(pose, nose)\n",
    "    lh_adjusted = adjust_landmarks(lh, lh_wrist)\n",
    "    rh_adjusted = adjust_landmarks(rh, rh_wrist)\n",
    "    return pose_adjusted, lh_adjusted, rh_adjusted\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (embedding): Linear(in_features=225, out_features=225, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=225, out_features=225, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=225, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=225, bias=True)\n",
       "        (norm1): LayerNorm((225,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((225,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=225, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определение модели и загрузка весов\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, num_heads=5, dim_feedforward=128, num_layers=2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, input_size)  # линейный слой для преобразования размерности входных данных\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0, 2)  # Transformer ожидает размерность (seq_len, batch, features)\n",
    "        x = self.embedding(x)  # Преобразование размерности входных данных\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=0)  # Усреднение по всем временным шагам\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "input_size = 225\n",
    "num_classes = 100\n",
    "model = TransformerModel(input_size, num_classes)\n",
    "model.load_state_dict(torch.load('best_model_transformer_v1.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инференс на видео и сохранение результата\n",
    "def run_inference_on_video(video_path, output_video_path, model, holistic, num_frames=20):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (height, height))\n",
    "\n",
    "    keypoints_sequence = []\n",
    "\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_indices = np.linspace(0, total_frames - 1, min(num_frames, total_frames), dtype=int)\n",
    "\n",
    "        frame_index = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or frame_index > frame_indices[-1]:\n",
    "                break\n",
    "\n",
    "            if frame_index in frame_indices:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                h, w, _ = frame_rgb.shape\n",
    "                if h > w:\n",
    "                    padding = (h - w) // 2\n",
    "                    frame_rgb = cv2.copyMakeBorder(frame_rgb, 0, 0, padding, padding, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "                else:\n",
    "                    padding = (w - h) // 2\n",
    "                    frame_rgb = cv2.copyMakeBorder(frame_rgb, padding, padding, 0, 0, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "\n",
    "                results = holistic.process(frame_rgb)\n",
    "                pose_adjusted, lh_adjusted, rh_adjusted = extract_keypoints(results)\n",
    "                keypoints_sequence.append(np.concatenate([pose_adjusted, lh_adjusted, rh_adjusted]))\n",
    "\n",
    "                frame_rgb = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "                draw_styled_landmarks(frame_rgb, results)\n",
    "                out.write(frame_rgb)\n",
    "\n",
    "            frame_index += 1\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "\n",
    "    while len(keypoints_sequence) < num_frames:\n",
    "        keypoints_sequence.append(np.zeros(33*3 + 21*3 + 21*3))\n",
    "\n",
    "    keypoints_sequence = np.array(keypoints_sequence)\n",
    "    keypoints_sequence = torch.tensor(keypoints_sequence, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(keypoints_sequence)\n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_data2 = pd.read_csv(os.path.join(DATA_DIR, 'class_data_3.csv'))\n",
    "class_data2 = class_data2.set_index('label')['class_name'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: день\n"
     ]
    }
   ],
   "source": [
    "# Пример использования\n",
    "video_path = './день_cut.mp4'\n",
    "output_video_path = './день_cut_inference.mp4'\n",
    "predicted_class = run_inference_on_video(video_path, output_video_path, model, mp_holistic)\n",
    "print(f'Predicted Class: {class_data2[predicted_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: благодарю\n"
     ]
    }
   ],
   "source": [
    "video_path = './благодарю_cut.mp4'\n",
    "output_video_path = './благодарю_cut_inference.mp4'\n",
    "predicted_class = run_inference_on_video(video_path, output_video_path, model, mp_holistic)\n",
    "print(f'Predicted Class: {class_data2[predicted_class]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "it_academy_fp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
